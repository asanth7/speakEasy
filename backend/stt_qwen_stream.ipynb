{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023f05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from uuid import uuid4\n",
    "\n",
    "import modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b05139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_TYPE = os.environ.get(\"GPU_TYPE\", \"l40s\")\n",
    "GPU_COUNT = os.environ.get(\"GPU_COUNT\", 1)\n",
    "\n",
    "GPU_CONFIG = f\"{GPU_TYPE}:{GPU_COUNT}\"\n",
    "\n",
    "SGL_LOG_LEVEL = \"error\"  # try \"debug\" or \"info\" if you have issues\n",
    "\n",
    "MINUTES = 60  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df1ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "MODEL_REVISION = \"a7a06a1cc11b4514ce9edcde0e3ca1d16e5ff2fc\"\n",
    "TOKENIZER_PATH = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "MODEL_CHAT_TEMPLATE = \"qwen2-vl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe3cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VOL_PATH = Path(\"/models\")\n",
    "MODEL_VOL = modal.Volume.from_name(\"sgl-cache\", create_if_missing=True)\n",
    "volumes = {MODEL_VOL_PATH: MODEL_VOL}\n",
    "\n",
    "\n",
    "def download_model():\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    snapshot_download(\n",
    "        MODEL_PATH,\n",
    "        local_dir=str(MODEL_VOL_PATH / MODEL_PATH),\n",
    "        revision=MODEL_REVISION,\n",
    "        ignore_patterns=[\"*.pt\", \"*.bin\"],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48d3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_version = \"12.8.0\"  # should be no greater than host CUDA version\n",
    "flavor = \"devel\"  #  includes full CUDA toolkit\n",
    "operating_sys = \"ubuntu22.04\"\n",
    "tag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n",
    "\n",
    "vlm_image = (\n",
    "    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.11\")\n",
    "    .entrypoint([])  # removes chatty prints on entry\n",
    "    .apt_install(\"libnuma-dev\")  # Add NUMA library for sgl_kernel\n",
    "    .pip_install(  # add sglang and some Python dependencies\n",
    "        \"transformers==4.54.1\",\n",
    "        \"numpy<2\",\n",
    "        \"fastapi[standard]==0.115.4\",\n",
    "        \"pydantic==2.9.2\",\n",
    "        \"requests==2.32.3\",\n",
    "        \"starlette==0.41.2\",\n",
    "        \"torch==2.7.1\",\n",
    "        \"sglang[all]==0.4.10.post2\",\n",
    "        \"sgl-kernel==0.2.8\",\n",
    "        \"hf-xet==1.1.5\",\n",
    "    )\n",
    "    .env(\n",
    "        {\n",
    "            \"HF_HOME\": str(MODEL_VOL_PATH),\n",
    "            \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n",
    "        }\n",
    "    )\n",
    "    .run_function(  # download the model by running a Python function\n",
    "        download_model, volumes=volumes\n",
    "    )\n",
    "    .pip_install(  # add an optional extra that renders images in the terminal\n",
    "        \"term-image==0.7.1\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88a3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = modal.App(\"example-sgl-vlm\")\n",
    "\n",
    "\n",
    "@app.cls(\n",
    "    gpu=GPU_CONFIG,\n",
    "    timeout=20 * MINUTES,\n",
    "    scaledown_window=20 * MINUTES,\n",
    "    image=vlm_image,\n",
    "    volumes=volumes,\n",
    ")\n",
    "@modal.concurrent(max_inputs=100)\n",
    "class Model:\n",
    "    @modal.enter()  # what should a container do after it starts but before it gets input?\n",
    "    def start_runtime(self):\n",
    "        \"\"\"Starts an SGL runtime to execute inference.\"\"\"\n",
    "        import sglang as sgl\n",
    "\n",
    "        self.runtime = sgl.Runtime(\n",
    "            model_path=MODEL_PATH,\n",
    "            tokenizer_path=TOKENIZER_PATH,\n",
    "            tp_size=GPU_COUNT,  # t_ensor p_arallel size, number of GPUs to split the model over\n",
    "            log_level=SGL_LOG_LEVEL,\n",
    "        )\n",
    "        self.runtime.endpoint.chat_template = sgl.lang.chat_template.get_chat_template(\n",
    "            MODEL_CHAT_TEMPLATE\n",
    "        )\n",
    "        sgl.set_default_backend(self.runtime)\n",
    "\n",
    "    @modal.fastapi_endpoint(method=\"POST\", docs=True)\n",
    "    def generate(self, request: dict) -> str:\n",
    "        from pathlib import Path\n",
    "\n",
    "        import requests\n",
    "        import sglang as sgl\n",
    "        from term_image.image import from_file\n",
    "\n",
    "        start = time.monotonic_ns()\n",
    "        request_id = uuid4()\n",
    "        print(f\"Generating response to request {request_id}\")\n",
    "\n",
    "        image_url = request.get(\"image_url\")\n",
    "        if image_url is None:\n",
    "            image_url = (\n",
    "                \"https://modal-public-assets.s3.amazonaws.com/golden-gate-bridge.jpg\"\n",
    "            )\n",
    "\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        image_filename = image_url.split(\"/\")[-1]\n",
    "        image_path = Path(f\"/tmp/{uuid4()}-{image_filename}\")\n",
    "        image_path.write_bytes(response.content)\n",
    "\n",
    "        @sgl.function\n",
    "        def image_qa(s, image_path, question):\n",
    "            s += sgl.user(sgl.image(str(image_path)) + question)\n",
    "            s += sgl.assistant(sgl.gen(\"answer\"))\n",
    "\n",
    "        question = request.get(\"question\")\n",
    "        if question is None:\n",
    "            question = \"What is this?\"\n",
    "\n",
    "        state = image_qa.run(\n",
    "            image_path=image_path, question=question, max_new_tokens=128\n",
    "        )\n",
    "        # show the question and image in the terminal for demonstration purposes\n",
    "        print(Colors.BOLD, Colors.GRAY, \"Question: \", question, Colors.END, sep=\"\")\n",
    "        terminal_image = from_file(image_path)\n",
    "        terminal_image.draw()\n",
    "        print(\n",
    "            f\"request {request_id} completed in {round((time.monotonic_ns() - start) / 1e9, 2)} seconds\"\n",
    "        )\n",
    "\n",
    "        return state[\"answer\"]\n",
    "\n",
    "    @modal.exit()  # what should a container do before it shuts down?\n",
    "    def shutdown_runtime(self):\n",
    "        self.runtime.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5ebac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1614932489.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    modal run sgl_vlm.py\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
